{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78acabdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pkg_resources\n",
    "import json\n",
    "import itertools\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from convokit import Corpus, download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af1bd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(filename=download(\"conversations-gone-awry-corpus\"))\n",
    "len(corpus.utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15135ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124692bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_ids = []\n",
    "convo_ids = []\n",
    "timestamps = []\n",
    "page_ids = []\n",
    "awry = []\n",
    "context = []\n",
    "for comment_id in corpus.utterances:\n",
    "    comment = corpus.utterances[comment_id]\n",
    "    # section headers are included in the dataset for completeness, but for prediction we need to ignore\n",
    "    # them as they are not utterances\n",
    "    if not comment.meta[\"is_section_header\"]:\n",
    "        comment_ids.append(comment_id)\n",
    "        convo_ids.append(comment.conversation_id)\n",
    "        timestamps.append(comment.timestamp)\n",
    "        #page_ids.append(comment.meta[\"awry_info\"][\"page_id\"])\n",
    "        #awry.append(comment.meta[\"awry_info\"][\"conversation_has_personal_attack\"])\n",
    "        context.append(comment.reply_to)\n",
    "comment_df = pd.DataFrame({\"conversation_id\": convo_ids, \"timestamp\": timestamps, \"awry\": True, \"context\":context}, index=comment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab854b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe04936",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df[comment_df.context==\"None\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771daea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_distinct_df = comment_df.drop_duplicates(subset=['conversation_id']).drop(['timestamp'],axis=1)\n",
    "conv_distinct_df = pd.DataFrame({'conversation_id': list(comm_distinct_df['conversation_id']), 'awry': list(comm_distinct_df['awry'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987ba557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237e67d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = []\n",
    "conversations_dict = {}\n",
    "\n",
    "for index, row in comment_df.iterrows():\n",
    "    if row['conversation_id'] in conversations_dict.keys():\n",
    "        conversations_dict[row['conversation_id']].append(index)\n",
    "    else:\n",
    "        conversations_dict[row['conversation_id']]=[index]\n",
    "        \n",
    "for index, row in conv_distinct_df.iterrows():\n",
    "    conversations.append({'conversation_id': row['conversation_id'], 'utterances': [i for i in conversations_dict[row['conversation_id']]], 'awry': row['awry']})\n",
    "    \n",
    "conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc270b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([t for i in conversations for t in i['tokens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e1599d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus.utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a19f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update([\"n't\",\"'s\",\"'m\"])\n",
    "punctuations=\"?:!.,;'[]{}\\/-|`<>*()_\"\n",
    "\n",
    "for i in conversations:\n",
    "    for j in i['utterances']:\n",
    "        if not 'texts' in i:\n",
    "            i['texts'] = [corpus.utterances[j].text]\n",
    "        else:\n",
    "            i['texts'].append(corpus.utterances[j].text)\n",
    "conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74aedb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "for i in conversations:\n",
    "    for j in i['texts']:\n",
    "        filtered_sentence = []\n",
    "        words = nltk.word_tokenize(j)\n",
    "        for w in words:\n",
    "            if((w not in stop_words) and (w not in punctuations)):\n",
    "                filtered_sentence.append(w)\n",
    "        result = [wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in filtered_sentence]\n",
    "        if not 'tokens' in i:\n",
    "            i['tokens'] = [result]\n",
    "        else:\n",
    "            i['tokens'].append(result)\n",
    "conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0069fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "emowordnet = pd.read_csv(\"emowordnet.csv\", sep=';')\n",
    "emowordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db879d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in conversations:\n",
    "    maxlist = []\n",
    "    avglist = []\n",
    "    for j in i[\"tokens\"]:\n",
    "        emostats = [0,0,0,0,0,0,0,0]\n",
    "        emomax = 0\n",
    "        length = len(j)\n",
    "        for word in j:\n",
    "            result = emowordnet[emowordnet['Lemma']==word.lower()]\n",
    "            if(len(result)!=0):\n",
    "                result = result.iloc[0]\n",
    "                emolist = [result['AFRAID'], result['AMUSED'], result['ANGRY'], result['ANNOYED'], result['DONT_CARE'], result['HAPPY'], result['INSPIRED'], result['SAD']]\n",
    "                emostats = [x + y for x, y in zip(emostats, emolist)]\n",
    "                if max(emolist)>emomax:\n",
    "                    emomax = max(emolist)\n",
    "            else:\n",
    "                length-=1\n",
    "        maxlist.append(emomax)\n",
    "        avglist.append(max(emolist)/max(length,1))\n",
    "    i['max'] = maxlist\n",
    "    i['avg'] = avglist\n",
    "\n",
    "conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca6a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ab5e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('conversations_time_series.json', 'w') as file:\n",
    "    json.dump(conversations, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f328da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(index, row, context_df):\n",
    "    context = []\n",
    "    indices = list(context_df.index)\n",
    "    if row[\"context\"] in indices:\n",
    "        reply_to = row[\"context\"]\n",
    "        context.append(reply_to)\n",
    "        new_row = context_df.loc[reply_to]\n",
    "        print(reply_to,new_row)\n",
    "        context.extend(get_context(reply_to, new_row, context_df))\n",
    "    else:\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b058b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(comment_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d573e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_context_dict = {}\n",
    "for index, row in comment_df.iterrows():\n",
    "    conversations_context_dict[index] = []\n",
    "    \n",
    "for index, row in comment_df.iterrows():\n",
    "    #print(index,conversations_context_dict[reply_to],reply_to)\n",
    "    if row[\"context\"] not in conversations_context_dict.keys():\n",
    "        conversations_context_dict[index] = []\n",
    "    else:\n",
    "        conversations_context_dict[index] = [conversations_context_dict[row[\"context\"]],row[\"context\"]]\n",
    "conversations_context_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1cc7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "def flatten(l):\n",
    "    for el in l:\n",
    "        if isinstance(el, Iterable) and not isinstance(el, (str, bytes)):\n",
    "            yield from flatten(el)\n",
    "        else:\n",
    "            yield el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2415f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in conversations_context_dict.items():\n",
    "    conversations_context_dict[k] = list(flatten(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a29921",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_context_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68ec5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_context = []\n",
    "for k,v in conversations_context_dict.items():\n",
    "    #print(v,k)\n",
    "    conversations_context.append(v+[k])\n",
    "conversations_context    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668542f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_element = []\n",
    "for i in conversations_context:\n",
    "    if(len(i)==1):\n",
    "        one_element.append(i)\n",
    "#print(len(conversations_context),len(one_element))\n",
    "conversations_context = ([item for item in conversations_context if item not in one_element])\n",
    "len(conversations_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc79e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e2b071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subfinder(mylist, pattern):\n",
    "    matches = []\n",
    "    for i in range(len(mylist)):\n",
    "        if mylist[i] == pattern[0] and mylist[i:i+len(pattern)] == pattern:\n",
    "            matches.append(pattern)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e626eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = []\n",
    "for i in range(1,len(conversations_context)):\n",
    "    if len(subfinder(conversations_context[i],conversations_context[i-1]))!=0: \n",
    "        to_remove.append(conversations_context[i-1]) \n",
    "conversations_context = [e for e in conversations_context if e not in to_remove]\n",
    "conversations_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cf9d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_context_list = []\n",
    "for i in conversations_context:\n",
    "    for j in conversations:\n",
    "        if i[0] in j['utterances']:\n",
    "            conversations_context_list.append({\"conversation_id\": j['conversation_id'], \"awry\": j[\"awry\"],\n",
    "                                              \"discussion\": i, \n",
    "                                              \"max\": [j['max'][j['utterances'].index(k)] for k in j['utterances']\n",
    "                                                      for l in i if k==l], \n",
    "                                              \"avg\": [j['avg'][j['utterances'].index(k)] for k in j['utterances']\n",
    "                                                      for l in i if k==l]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f428aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f930f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('conversations_context_time_series.json', 'w') as file:\n",
    "    json.dump(conversations_context_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba67793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
